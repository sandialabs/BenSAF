{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd05535f",
   "metadata": {},
   "source": [
    "# BenSAF Workflow Example\n",
    "This notebook demonstrates the use of BenSAF to execute a health benefits analysis on the use of alternative aviation fuels. The notebook reproduces a simplified version of the ORD analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f47cdd7",
   "metadata": {},
   "source": [
    "## 1. Initialization\n",
    "Import modules and set options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849a669d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import bensaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65a48c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_SYNTHETIC_DATA = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f599a75b",
   "metadata": {},
   "source": [
    "Create output directory and load tracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df96a640",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"output\", \"saf_workflow_example\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f650a3db",
   "metadata": {},
   "source": [
    "Load in tract data, exposure data, and mortality data. If `USE_SYNTHETIC_DATA` is true random  values for the necessary columns are generated, otherwise real data is loaded (assuming it is available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0b2a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_SYNTHETIC_DATA:\n",
    "    tracts_file = Path(\"..\", \"sources\", \"melissa_ord\", \"data\", \"Illinois Shapefile Bounded\")\n",
    "    tracts = gpd.read_file(tracts_file)\n",
    "    tracts_gdf, exposure_df, mortality_df = bensaf.utils.create_synthetic_data(\n",
    "        tracts_gdf=tracts\n",
    "    )\n",
    "else: # Load from processed data\n",
    "    tracts_gdf = gpd.read_file('../data/converted/tracts_gdf.geojson')\n",
    "    exposure_df = pd.read_csv('../data/converted/exposure_df.csv')\n",
    "    mortality_df = pd.read_csv('../data/converted/mortality_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784da722",
   "metadata": {},
   "source": [
    "Preview tract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db5ed51",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracts_gdf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb12ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracts_gdf.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5517fdd",
   "metadata": {},
   "source": [
    "Preview exposure data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282b5c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "exposure_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6b1d00",
   "metadata": {},
   "source": [
    "Preview mortality data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6f0b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mortality_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e241e1",
   "metadata": {},
   "source": [
    "Define the workflow configuration. This configuration file contains various settings and parameters used by the workflow class to execute the analysis. The config can alternatively be loaded from a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536e84ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'control_scenarios': [5, 25, 50],\n",
    "    'demographic_columns': ['poc_proportion', 'low_income_proportion'],\n",
    "    'airport_coordinates': (-87.90472, 41.97861)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7117bd06",
   "metadata": {},
   "source": [
    "Initialize the workflow class using the config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc44d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = bensaf.workflow.Workflow(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf7398c",
   "metadata": {},
   "source": [
    "## 2. Process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c1022b",
   "metadata": {},
   "source": [
    "Add the data to the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a416ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.load_tract_data(tracts_gdf)\n",
    "workflow.load_exposure_data(exposure_df)\n",
    "workflow.load_mortality_data(mortality_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97cf5e9",
   "metadata": {},
   "source": [
    "Use the `prepare_data` method to combine the mortality and exposure data with the tract data to create a unified GeoDataFrame accessed at `workflow.analysis_data`.\n",
    "\n",
    "Additionally, some initial data preprocessing is performed such as computing natural mortality rate per captia from the natural mortality rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4941c919",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.prepare_data()\n",
    "workflow.analysis_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273b0465",
   "metadata": {},
   "source": [
    "Add discrete distance categories to the tracts using the `bin_tracts_by_distance` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31f368c",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.bin_tracts_by_distance([0, 10, 20, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d065232",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.analysis_data.plot(\"distance_bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc6dbd3",
   "metadata": {},
   "source": [
    "Aggregated statistics can be easily accessed using Pandas' `groupby` method. Total population within each distance bin category is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71e31ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.analysis_data.groupby(\"distance_bin\")[\"population\"].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5765a78",
   "metadata": {},
   "source": [
    "Plot boxplots of the tract populations within each distance bin. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110d2c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop10km =workflow.analysis_data.loc[workflow.analysis_data[\"distance_bin\"] == \"0-10 km\", \"population\"]\n",
    "pop1020km =workflow.analysis_data.loc[workflow.analysis_data[\"distance_bin\"] == \"10-20 km\", \"population\"]\n",
    "pop20km =workflow.analysis_data.loc[workflow.analysis_data[\"distance_bin\"] == \"20+ km\", \"population\"]\n",
    "\n",
    "bp = plt.boxplot([pop10km, pop1020km, pop20km], positions = [1, 2, 3], widths=0.95, showfliers=False, patch_artist=True);\n",
    "\n",
    "plt.xlim([-0.1, 4.1])\n",
    "plt.xticks([1, 2, 3], ['<10 km', '10-20 km', '>20 km'])\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "plt.title('Tract Population by Distance Bin', fontsize=15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e6f498",
   "metadata": {},
   "source": [
    "The `create_choropleth_map` function can be used to plot variables geospatially across tracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034e71cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bensaf.graphics import create_choropleth_map\n",
    "\n",
    "create_choropleth_map(\n",
    "    workflow.analysis_data, \n",
    "    \"population\", \n",
    "    \"Total Population\",\n",
    "    point_locations=workflow.config['airport_coordinates'],\n",
    "    vmin=0,\n",
    "    vmax=10000,\n",
    "    cmap='Blues',\n",
    "    show_background=True,\n",
    "    show_boundaries=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544cdf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_choropleth_map(\n",
    "    workflow.analysis_data, \n",
    "    \"low_income_proportion\", \n",
    "    \"Low Income Proportion\",\n",
    "    point_locations=workflow.config['airport_coordinates'],\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    "    cmap='Reds',\n",
    "    show_background=True,\n",
    "    show_boundaries=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df5844c",
   "metadata": {},
   "source": [
    "## 3. Execute Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194abac9",
   "metadata": {},
   "source": [
    "### Apply control scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8554948c",
   "metadata": {},
   "source": [
    "The `apply_control_scenarios` method uses the scenario percentages defined in config to generate UFP reduction scenarios. This produces a dictionary of scenarios `workflow.control_scenarios`. The \"data\" key of each scenario dictionary contains a dataframe with the reduced and delta concentrations for each tract. Later in the analysis, other scenario relevant values are added to this dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5be626",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.apply_control_scenarios()\n",
    "workflow.control_scenarios[5] # 5 reduction scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d2b870",
   "metadata": {},
   "source": [
    "Plot pollutant concentration maps for the baseline and three reduction cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782b35de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2x2 grid of maps using choropleth_map\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Define the variables to map\n",
    "control_scenarios = workflow.control_scenarios\n",
    "variables = [\n",
    "    (workflow.analysis_data[\"pollutant_concentration\"], \"Baseline\"),\n",
    "    (control_scenarios[5][\"data\"][\"reduced_concentration_5\"], \"5% Reduction\"),\n",
    "    (control_scenarios[25][\"data\"][\"reduced_concentration_25\"], \"25% Reduction\"),\n",
    "    (control_scenarios[50][\"data\"][\"reduced_concentration_50\"], \"50% Reduction\"),\n",
    "]\n",
    "\n",
    "# Calculate global min and max for consistent color scaling\n",
    "all_values = []\n",
    "for column_data, _ in variables:\n",
    "    all_values.extend(column_data.values)\n",
    "\n",
    "global_vmin = min(all_values)\n",
    "global_vmax = max(all_values)\n",
    "\n",
    "print(f\"Global color range: {global_vmin:.2f} to {global_vmax:.2f}\")\n",
    "\n",
    "# Create each map\n",
    "for i, (column_data, title) in enumerate(variables):\n",
    "    row = i // 2\n",
    "    col = i % 2\n",
    "    ax = axs[row, col]\n",
    "    \n",
    "    # Use create_choropleth_map function\n",
    "    bensaf.graphics.create_choropleth_map(\n",
    "        gdf=workflow.analysis_data,\n",
    "        column=column_data,\n",
    "        title=title,\n",
    "        vmin=global_vmin,\n",
    "        vmax=global_vmax,\n",
    "        point_locations=workflow.config['airport_coordinates'],\n",
    "        ax=ax\n",
    "    )\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce90639e",
   "metadata": {},
   "source": [
    "### Calculate health impacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f5f61a",
   "metadata": {},
   "source": [
    "The `calculate_health_impacts` method uses the default health impact function from Bouma et al to compute:\n",
    "- Relative risk\n",
    "- Attributable fraction\n",
    "- Attributable cases\n",
    "- Attributable mortality\n",
    "\n",
    "for each scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d74430",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.calculate_health_impacts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f1717c",
   "metadata": {},
   "source": [
    "After running the method, the `control_scenarios` dictionaries  will be populated with health impact data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffcaa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.control_scenarios[5].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c3eab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.control_scenarios[25]['attributable_cases_25']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3231132",
   "metadata": {},
   "source": [
    "## 4. Explore results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5451797",
   "metadata": {},
   "source": [
    "Print study population characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a47a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Study Population: '+str(round(np.sum(workflow.analysis_data['population']))))\n",
    "\n",
    "print('Low Income Population: '+str(round(np.sum(workflow.analysis_data['low_income_population']))))\n",
    "print('Low Income Population Proportion: '+str(round(100*np.sum(workflow.analysis_data['low_income_population'])/np.sum(workflow.analysis_data['population']), 2)))\n",
    "\n",
    "print('POC Population: '+str(round(np.sum(workflow.analysis_data['poc_population']))))\n",
    "print('POC Population Proportion: '+str(round(100*np.sum(workflow.analysis_data['poc_population'])/np.sum(workflow.analysis_data['population']), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f35745a",
   "metadata": {},
   "source": [
    "Print weighted UFP values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc909431",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Average UFP: '+str(round(workflow.analysis_data['pollutant_concentration'].mean(), 1))+' pt/cm\\u00b3')\n",
    "\n",
    "print('Low Income Population-Weighted UFP: '+str(round(bensaf.utils.calculate_weighted_ufp(workflow.analysis_data['pollutant_concentration'], workflow.analysis_data['low_income_population']), 1))+' pt/cm\\u00b3')\n",
    "print('Not Low Income Population-Weighted UFP: '+str(round(bensaf.utils.calculate_weighted_ufp(workflow.analysis_data['pollutant_concentration'], workflow.analysis_data['not_low_income_population']), 1))+ ' pt/cm\\u00b3')\n",
    "\n",
    "print('POC Population-Weighted UFP: '+str(round(bensaf.utils.calculate_weighted_ufp(workflow.analysis_data['pollutant_concentration'], workflow.analysis_data['poc_population']), 1))+' pt/cm\\u00b3')\n",
    "print('Non POC Population-Weighted UFP: '+str(round(bensaf.utils.calculate_weighted_ufp(workflow.analysis_data['pollutant_concentration'], workflow.analysis_data['population']-workflow.analysis_data['poc_population']), 1))+ ' pt/cm\\u00b3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a050d23",
   "metadata": {},
   "source": [
    "Print Attributable case reductions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4081df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Attributable Case Reductions for Entire Population (5% SAF Blend): '+str(round(workflow.control_scenarios[5]['attributable_cases_5']))+' ('+str(round(workflow.control_scenarios[5]['lower_attributable_cases_5']))+', '+str(round(workflow.control_scenarios[5]['upper_attributable_cases_5']))+')')\n",
    "print('Attributable Case Reductions for Entire Population (25% SAF Blend): '+str(round(workflow.control_scenarios[25]['attributable_cases_25']))+' ('+str(round(workflow.control_scenarios[25]['lower_attributable_cases_25']))+', '+str(round(workflow.control_scenarios[25]['upper_attributable_cases_25']))+')')\n",
    "print('Attributable Case Reductions for Entire Population (50% SAF Blend): '+str(round(workflow.control_scenarios[50]['attributable_cases_50']))+' ('+str(round(workflow.control_scenarios[50]['lower_attributable_cases_50']))+', '+str(round(workflow.control_scenarios[50]['upper_attributable_cases_50']))+')')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saf_toolkit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
